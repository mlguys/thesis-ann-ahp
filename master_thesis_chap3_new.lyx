#LyX 2.2 created this file. For more info see http://www.lyx.org/
\lyxformat 508
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{url}
\setkeys{Gin}{width=\ifdim\Gin@nat@width>\linewidth
  \linewidth
\else
  \Gin@nat@width
\fi}
\makeatother
\usepackage{pgfplots}
\end_preamble
\use_default_options true
\master master_thesis.lyx
\maintain_unincluded_children false
\begin_local_layout
Format 60
CiteEngine authoryear
Citep*[][]
Citet*[][]
Citealt*[][]
Citealp*[][]
Citeauthor*[]
citeyear[]
citeyearpar[][]
nocite
End
\end_local_layout
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement !tph
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Section
Artificial Neural Network
\begin_inset CommandInset label
LatexCommand label
name "sec:Artificial-Neural-Network"

\end_inset


\end_layout

\begin_layout Standard
When using Multi-Criteria Decision Analysis, especially AHP method, to deal
 with decision problems which have many criteria or alternatives, the cost
 of computation to find the best possible decision could increase exponentially.
 Also, the task in which a decision maker have to make the best possible
 decision as an output from a system where it takes existing information,
 and future predictions as the input is a challenging task 
\begin_inset CommandInset citation
LatexCommand citep
key "golmohammadi2011neural"

\end_inset

.
\end_layout

\begin_layout Standard
However, there is an interesting approach to approximate the best possible
 decision from such system without paying too much computation cost; it
 is by using Artificial Neural Network (ANN).
\end_layout

\begin_layout Standard
ANN is an alternative approach to computing which emulates the remarkable
 ability of the human mind in reasoning and learning in an environment of
 uncertainty and imprecision.
 In fact, ANN simulates the human brain and its ability to learn, recall,
 and generalize from training data by modeling the essence of the human
 brain: the networks of biological neurons 
\begin_inset CommandInset citation
LatexCommand citep
key "mashrei2012neural"

\end_inset

.
 These features make ANN a powerful data modeling tool that is capable of
 capturing and representing the complex relationships between input and
 output data 
\begin_inset CommandInset citation
LatexCommand citep
key "golmohammadi2011neural"

\end_inset

.
\end_layout

\begin_layout Standard
With such properties, ANN has found its place in many applications such
 as function approximation, regression analysis, classification, data processing
, robotics, and control engineering 
\begin_inset CommandInset citation
LatexCommand citep
key "wiki:ann"

\end_inset

.
\end_layout

\begin_layout Standard
Therefore, in this section, we will get into the details of ANN, what inspires
 the model of ANN, what is the fundamental element of ANN, the architecture
 of ANN, how ANN can learn, and how to train ANN.
\end_layout

\begin_layout Subsection
The Biological Neuron
\begin_inset CommandInset label
LatexCommand label
name "subsec:The-Model-of"

\end_inset


\end_layout

\begin_layout Standard
In a sense, artificial neural network (ANN) simulate the function of the
 human brain.
 It is a type of network which nodes are artificial neurons.
 These artificial neurons are based on the mathematic model of biological
 neurons.
\end_layout

\begin_layout Standard
Biologically, a human brain is a network of about one hundred billion neurons,
 and each neuron connects to about ten thousand other neurons by using dendrites
, and axons.
 Every single neuron receives electrochemical input signals from other neurons
 at the dendrites.
 Then, the cell body of the neuron will sum all the strength of all the
 electrochemical input signals which it receives from other neurons, and
 if the sum is greater than some threshold level, the neuron will be activated.
 When the neuron is activated, it will transmit or fire an electrochemical
 signal along the axon and pass this signal to the dendrites of other neurons
 that are attached to this neuron.
 It is important to know that a neuron does not fire any signal if the sum
 of electrochemical inputs does not surpass a particular level.
 In other words, a neuron can only do two actions: fire a signal or not,
 there is no other action in between 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann"

\end_inset

.
 The structure of a biological neuron is shown in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:A-Biological-Neuron"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/biological neuron mashrei.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:A-Biological-Neuron"

\end_inset

A Biological Neuron 
\begin_inset CommandInset citation
LatexCommand citep
key "mashrei2012neural"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
Hence, the entire human brain is a huge network of interconnected electrochemica
l transmitting neurons.
 This property gives the human brain the power to perform exceptionally
 complex tasks.
 Because of this power, the human brain has become the model for the ANN
 to be based on in order to solve problems which are simple tasks for an
 ordinary human but challenging for a conventional computer.
 Some notable examples of these problems are image recognition and predictions
 based on past knowledge 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann"

\end_inset

.
\end_layout

\begin_layout Subsection
The Artificial Neuron
\begin_inset CommandInset label
LatexCommand label
name "subsec:Artificial-Neuron"

\end_inset


\end_layout

\begin_layout Standard
To model ANN from the human brain, first, we have to model the most fundamental
 element, the biological neuron.
 Similar to the natural counterpart, an artificial neuron is a simple processing
 unit which consists of numerical input values (the receiving electrochemical
 input signals), which are multiplied by weights (the strength of the respective
 electrochemical input signals), and these inputs will then be totaled by
 a sum function.
 Finally, an activation function will calculate the output of the artificial
 neuron by using the result of the sum function 
\begin_inset CommandInset citation
LatexCommand citep
key "mashrei2012neural"

\end_inset

.
 A typical artificial neuron is shown in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:An-artificial-neuron"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/perceptron.jpg

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:An-artificial-neuron"

\end_inset

An artificial neuron 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann2"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:An-artificial-neuron"

\end_inset

, we can see that an artificial neuron has many inputs 
\begin_inset Formula $\mbox{x_{1,}x_{2,}x_{3},...,x_{n}}$
\end_inset

 and each input is independently weighted 
\begin_inset Formula $\mbox{w_{1,}w_{2,}w_{3},...,w_{n}}$
\end_inset

 when we calculate the sum of all input signals.
 These weights have the responsibility to amplify or weaken the original
 input signals.
 For example, if the input 
\begin_inset Formula $x_{1}$
\end_inset

 has the initial value of 1 and the weight 
\begin_inset Formula $w_{1}$
\end_inset

 corresponding to 
\begin_inset Formula $x_{1}$
\end_inset

 has the value of 0.5, then the value we put into the sum function will be
 0.5 because 
\begin_inset Formula $x_{1}w_{1}=1*0.5=0.5$
\end_inset

.
 After all weighted input signals have been added together, the result will
 be passed into the activation function which will determine the output
 of the artificial neuron.
 There are several types of activation function such as step function, pure-line
ar function, log sigmoid function, or tangent sigmoid function 
\begin_inset CommandInset citation
LatexCommand citep
key "wiki:activationfunctions"

\end_inset

.
 The 
\begin_inset CommandInset ref
LatexCommand formatted
reference "tab:Activation-functions"

\end_inset

 compares the properties of those activation functions.
 The plots of those functions will be shown in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:The-plot-of"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:The-plot-of-1"

\end_inset

, 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:The-plot-of-2"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:The-plot-of-3"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Activation-functions"

\end_inset

Activation functions 
\begin_inset CommandInset citation
LatexCommand citep
key "wiki:activationfunctions"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="3">
<features booktabs="true" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Name
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Equation
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Output range
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Pure-linear (Identity)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $f(x)=x$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $(-\infty,\infty)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Step
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $f(x)=\begin{cases}
0 & x<T\\
1 & x\geq T
\end{cases}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $\{0,1\}$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Log sigmoid (Logistic)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $f(x)=\frac{1}{1+e^{-x}}$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $(0,1)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Tangent
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $f(x)=tanh(x)=\frac{2}{1+e^{-2x}}-1$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $(-1,1)$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture} [scale=1.0] 
\end_layout

\begin_layout Plain Layout

	
\backslash
begin{axis} [ ymax=2, ymin=-2, xlabel=$x$, ylabel=$f(x)$, width=10cm, axis
 on top=true, axis x line=middle, axis y line=middle ] 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
addplot [blue, line width = 2, smooth, domain=-2:2] {x}; 
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{axis} 
\backslash
end{tikzpicture}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:The-plot-of"

\end_inset

The plot of Pure-linear (Identity) function
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture} [scale=1.0] 
\backslash
begin{axis} [ ymax=1.5, ymin=-0.5, xlabel=$x$, ylabel=$f(x)$, width=10cm,
 axis on top=true, axis x line=middle, axis y line=middle ]
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
addplot [blue, line width = 2, smooth, domain=-1:0] {0}; 
\end_layout

\begin_layout Plain Layout


\backslash
addplot [blue, line width = 2, smooth, domain=0:1] {1};
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout


\backslash
end{axis} 
\backslash
end{tikzpicture}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:The-plot-of-1"

\end_inset

The plot of Step function with T = 0
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture} [scale=1.0] 
\backslash
begin{axis} [ ymax=1, ymin=0, xlabel=$x$, ylabel=$f(x)$, width=10cm, axis
 on top=true, axis x line=middle, axis y line=middle ] 
\backslash
addplot [blue, line width = 2, smooth, domain=-2:2] {1 / (1+exp(-x)}; 
\backslash
end{axis} 
\backslash
end{tikzpicture}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:The-plot-of-2"

\end_inset

The plot of Log sigmoid function
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
begin{tikzpicture} [scale=1.0] 
\backslash
begin{axis} [ ymax=1, ymin=-1, xlabel=$x$, ylabel=$f(x)$, width=10cm, axis
 on top=true, axis x line=middle, axis y line=middle ] 
\backslash
addplot [blue, line width = 2, smooth, domain=-2:2] {tanh(x)}; 
\backslash
end{axis} 
\backslash
end{tikzpicture}
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:The-plot-of-3"

\end_inset

The plot of Tangent function
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Lets us use the step function which is the simplest activation function
 in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "tab:Activation-functions"

\end_inset

 to demonstrate how an artificial neuron works.
 Typically, a step function will give the output of 1 if the input exceeds
 a specified threshold level, if not then the step function will produce
 the output of 0.
 For example, we have the following inputs, outputs and threshold level
 for an artificial neuron with the step function as its activation function:
\end_layout

\begin_layout Standard
\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Itemize
Input 
\begin_inset Formula $x_{1}=0.3$
\end_inset


\end_layout

\begin_layout Itemize
Input 
\begin_inset Formula $x_{2}=0.7$
\end_inset


\end_layout

\begin_layout Itemize
Weight 
\begin_inset Formula $w_{1}=1.3$
\end_inset


\end_layout

\begin_layout Itemize
Weight 
\begin_inset Formula $w_{2}=0.8$
\end_inset


\end_layout

\begin_layout Itemize
Threshold 
\begin_inset Formula $T=1.0$
\end_inset


\begin_inset VSpace defskip
\end_inset


\end_layout

\begin_layout Standard
According to 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:An-artificial-neuron"

\end_inset

, first we have to calculate the sum of all weighted input signals:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\sum=x_{1}w_{1}+x_{2}w_{2}=(0.3*1.3)+(0.7*0.8)=0.95
\]

\end_inset


\end_layout

\begin_layout Standard
Next, we input the calculated sum into the artificial neuron's activation
 function which is the step function.
 Because we have specified the threshold 
\begin_inset Formula $T=1.0$
\end_inset

 for the step function, the calculated sum does not exceed this threshold
 
\begin_inset Formula $T$
\end_inset

, therefore, the step function returns the output of 0, in other words,
 the artificial neuron does not fire.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\mbox{0.95<1.0 therefore f(0.95)=0}
\]

\end_inset


\end_layout

\begin_layout Subsection
Architecture of Artificial Neural Network
\begin_inset CommandInset label
LatexCommand label
name "subsec:Architecture-of-Artificial"

\end_inset


\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:Artificial-Neuron"

\end_inset

, we have taken a look at how artificial neuron works, now we will look
 at the architecture of ANN to see the way an ANN connects artificial neurons
 together and processes information.
\end_layout

\begin_layout Standard
Because the human brain is very complex, a single model of ANN can not cover
 all the functions of the human brain.
 Therefore, there are many types of ANN, each type of ANN deals with different
 aspects of the human brain such as classification or segmentation 
\begin_inset CommandInset citation
LatexCommand citep
key "wiki:anntypes"

\end_inset

.
 The followings are the most used types of ANN architecture 
\begin_inset CommandInset citation
LatexCommand citep
key "wiki:anntypes"

\end_inset

:
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000

\series bold
Feedforward
\begin_inset space ~
\end_inset

neural
\begin_inset space ~
\end_inset

network
\series default

\begin_inset CommandInset label
LatexCommand label
name "Feedforward-neural-network"

\end_inset

: This type of network is the most simple type of ANN, the flow of the informati
on moves in just one direction: begin from the input layer, information
 will be passed through the hidden layer and be transferred to the output
 layer.
 Also, there are no loops in the network; the calculation always goes forward,
 never goes back.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000

\series bold
Recurrent
\begin_inset space ~
\end_inset

neural
\begin_inset space ~
\end_inset

network
\series default
: Opposite to feedforward networks which information can only move from
 input to output, in recurrent neural networks, the information can go in
 a bi-directional flow.
 In other words, the information which has been processed in the later stages
 can be transferred back to earlier stages.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000

\series bold
Modular
\begin_inset space ~
\end_inset

neural
\begin_inset space ~
\end_inset

network
\series default
: Studies in biology have shown that the brain functions as a collection
 of small networks, therefore the concept modular neural network was conceived.
 In this concept, several artificial neural networks will try to cooperate
 or compete to solve problems.
\end_layout

\begin_layout Standard
In this section, we will discuss the detail of the feedforward neural network
 because it is the simplest neural network and also our primary ANN type
 for the case study of the thesis.
 First, let us consider the basic architecture of a feedforward neural network
 in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:Feedforward-neural-network"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/feedforward ann.jpg

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Feedforward-neural-network"

\end_inset

Feedforward neural network 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann2"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
As we can see, every node in the network is an artificial neuron described
 in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:Artificial-Neuron"

\end_inset

 and these nodes are arranged in layers 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann"

\end_inset

:
\end_layout

\begin_layout Description

\series bold
Input
\begin_inset space ~
\end_inset

layer
\series default
 which takes in inputs from the external world.
\end_layout

\begin_layout Description

\series bold
Hidden
\begin_inset space ~
\end_inset

layer
\series default
 which can be one hidden layer or multiple hidden layers and does not have
 any connection with the external world.
\end_layout

\begin_layout Description

\series bold
Output
\begin_inset space ~
\end_inset

layer
\series default
 which is responsible for producing the outputs.
\end_layout

\begin_layout Standard
In the input layer, because the artificial neurons only have the responsibility
 to bring the input from the external world into the network, therefore
 the artificial neurons in the input layer do not have any weight and will
 pass the same input it received from the external world to the next layer.
\end_layout

\begin_layout Standard
Each artificial neuron in a layer connects to every artificial neuron on
 the next layer.
 Thus, the information is always moving forward from one layer to the next
 layer.
 This characteristic is the reason why people call this type of artificial
 neural network 
\begin_inset Quotes eld
\end_inset

Feedforward
\begin_inset Quotes erd
\end_inset

 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann"

\end_inset

.
\end_layout

\begin_layout Standard
The readers should also notice that there is no connection between artificial
 neurons in the same layer.
\end_layout

\begin_layout Standard
\begin_inset Newpage newpage
\end_inset


\end_layout

\begin_layout Standard
Often, it is required for a Feedforward neural network or ANN, in general,
 to have a hidden layer in order to solve problems.
 The reason lies in 
\begin_inset Quotes eld
\end_inset

Linear separability
\begin_inset Quotes erd
\end_inset

 of the problem on which the ANN models 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann2"

\end_inset

.
 We will demonstrate the necessity of the hidden layer by modeling OR function
 and XOR function using ANN.
\end_layout

\begin_layout Standard
First, let us take a look at the graphs of OR function and XOR function
 in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:OR-function"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/or and xor function.jpg

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:OR-function"

\end_inset

OR function and XOR function
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann2"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
In the graph of OR function, we can see that the outputs of the OR function
 can be separated by a single straight line.
 This tells us OR function is 
\begin_inset Quotes eld
\end_inset

linearly separable
\begin_inset Quotes erd
\end_inset

, therefore it is possible to model the OR function using ANN without implementi
ng any hidden layer 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann2"

\end_inset

.
 In fact, OR function can be modeled with a single artificial neuron like
 in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:OR-function-modeled"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/or ann.jpg

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:OR-function-modeled"

\end_inset

OR function modeled by a single artificial neuron 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann2"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Standard
For the XOR function, looking at the graph we can see that it is impossible
 to separate the outputs of XOR function by just using only one straight
 line.
 Therefore, we have to use an additional hidden layer to achieve the separation
 of the outputs of XOR function 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann2"

\end_inset

.
 The extra hidden layer is added to the ANN model like in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:XOR-function-modeled"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/xor ann.jpg

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:XOR-function-modeled"

\end_inset

XOR function modeled with hidden layer 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann2"

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\begin_layout Subsection
The Learning Ability of Artificial Neural Network
\begin_inset CommandInset label
LatexCommand label
name "subsec:Learning"

\end_inset


\end_layout

\begin_layout Standard
In 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:Artificial-Neuron"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:Architecture-of-Artificial"

\end_inset

 we have been introduced to the basic concepts of ANN: what is an artificial
 neuron and how ANN connects artificial neurons to solve problems.
 Now we will explain the learning ability of ANN.
\end_layout

\begin_layout Standard
First, it is necessary to define what is the meaning of the word “learning”
 in the context of ANN.
 We can not say the way ANN learn is the same as how a human should learn
 because the possibility where machines can have awareness on what they
 are learning is still not yet decided.
 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann3"

\end_inset

.
 However, it is possible for ANN to get to perform better with experience.
 Therefore, we will define the learning process of ANN as follow: Learning
 in the context of ANN is the ability to perform better at a given task,
 or a range of tasks with the experience 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann3"

\end_inset

.
\end_layout

\begin_layout Standard
Now, let us recall from the beginning of 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:The-Model-of"

\end_inset

 which has shown us the way we modeled ANN from its natural counterpart,
 the human brain.
 Although ANN does not capture all functionalities, ANN has been able to
 model though simplified the most interesting characteristic of the human
 brain that is the ability to learn.
 During the learning process of the human brain, it is believed that the
 biological neural network of the brain is adjusted in a way that it either
 increases or decreases the strength of synaptic connections between neurons
 depending on the subject of the learning process 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann3"

\end_inset

.
 This property is the reason why human remembers relevant information easier
 than information that has not been used for a long time; it is because
 relevant information has stronger synaptic connections while less used
 information will have its synaptic connection weakened, therefore, making
 it difficult to remember 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann3"

\end_inset

.
\end_layout

\begin_layout Standard
To model the learning process of the human brain, ANN adjusts the weighted
 connections between artificial neurons in the network.
 This adjustment simulates the strengthening and weakening of synaptic connectio
ns in the human brain therefore it gives ANN the ability to learn 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann3"

\end_inset

.
\end_layout

\begin_layout Standard
The learning ability gives ANN many advantages over problems that have challenge
d tradition computer programs.
 For example, facial recognition problem is extremely hard for a programmer
 to code the right solution accurately; however, it can be solved much easier
 by using an ANN with its learning ability.
 ANN also can pick up the underlying relationship in input data.
 Therefore, ANN can solve classification problem where a loan granting applicati
on uses past loan data to classify future loan applications 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann3"

\end_inset

.
\end_layout

\begin_layout Subsection
Learning Paradigms
\begin_inset CommandInset label
LatexCommand label
name "subsec:Learning-Paradigms"

\end_inset


\end_layout

\begin_layout Standard
There are three major learning paradigms that can be used to train ANN,
 each paradigm has their advantages and disadvantages, but overall they
 share the same purpose: to find the best possible set of weights which
 ANN uses to accurately map any input to a correct output 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann3"

\end_inset

.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000

\series bold
Supervised
\begin_inset space ~
\end_inset

Learning
\series default
: In this paradigm, ANN is provided with the desired output along with the
 training input as a pair when in training.
 By using pairs of training input - desired output as training data, we
 can calculate an error value based on the difference between the output
 we want and the output produced by ANN.
 Then, we can use this error value to make corrections to the network by
 adjusting the network's weights, therefore, improve its performance.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000

\series bold
Unsupervised
\begin_inset space ~
\end_inset

Learning
\series default
: Opposite to supervised learning, ANN is only provided with a set of inputs,
 and it has to find the underlying pattern within the provided inputs without
 any outside intervention.
 Unsupervised learning is often used by data mining systems and other recommenda
tion systems because of its ability to predict preferences of users based
 on the preferences of other users in the same group.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000

\series bold
Reinforcement
\begin_inset space ~
\end_inset

Learning
\series default
: Instead of providing the desired output like in supervised learning, reinforce
ment learning introduces a reward system: a reward is awarded to the network
 based on its performance.
 The objective is to maximize the reward through trial-and-error.
 This kind of learning paradigm is similar to how animal learns in nature.
 For example, a dog is likely to remember the trick which the owner has
 given the dog the most candy (the reward in this case).
\end_layout

\begin_layout Subsection
Training Artificial Neural Network with Back-propagation
\begin_inset CommandInset label
LatexCommand label
name "subsec:Training-Artificial-Neural"

\end_inset


\end_layout

\begin_layout Standard
Back-propagation is a training method for the feed-forward artificial neural
 network.
 It belongs to the supervised learning paradigm where pairs of input and
 output are fed into the network for many cycles until the network can learn
 the relationship between the input and output 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann"

\end_inset

.
\end_layout

\begin_layout Standard
The method starts by applying the inputs as an input vector to the input
 layer of the network.
 Then, this input vector is passed or propagated through the hidden layer,
 and an output vector will be produced at the end of the network which is
 the output layer.
 When the forward propagation is completed, the network evaluates the errors
 between the output vector generated by the network and the desired outputs.
 Next, it uses the evaluated errors to adjust the weights of each artificial
 neuron in the network according to a learning rule which aims to minimize
 the error.
 Finally, the network uses the adjusted weights to start a new cycle.
 This back-propagation cycle or as call as an epoch will repeat until the
 errors between the desired outputs and the actual outputs from the network
 are minimized 
\begin_inset CommandInset citation
LatexCommand citep
key "mashrei2012neural"

\end_inset

.
 An diagram of a back-propagation cycle is depicted in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:Back-propagation-cycle"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename img/backprop.png

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Back-propagation-cycle"

\end_inset

Back-propagation cycle 
\begin_inset CommandInset citation
LatexCommand citep
key "mashrei2012neural"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
To demonstrate the back-propagation method, let us make a simple example
 in the classification problem.
 First, we provide the network with training data which consists of an input
 vector 
\begin_inset Formula $i$
\end_inset

 and its corresponding desired output 
\begin_inset Formula $d$
\end_inset

, the training data could be something like in 
\begin_inset CommandInset ref
LatexCommand formatted
reference "tab:Example-of-training"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "tab:Example-of-training"

\end_inset

Example of training data
\end_layout

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="5" columns="2">
<features booktabs="true" tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Input 
\begin_inset Formula $i$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Desired output 
\begin_inset Formula $d$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $(0,0)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $0$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $(0,1)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $1$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $(1,0)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $1$
\end_inset


\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $(1,1)$
\end_inset


\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
\begin_inset Formula $0$
\end_inset


\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
According to 
\begin_inset CommandInset ref
LatexCommand formatted
reference "fig:Back-propagation-cycle"

\end_inset

, every time we propagate the input vector 
\begin_inset Formula $i$
\end_inset

 through the network, we get the output vector 
\begin_inset Formula $o$
\end_inset

, we will then compare this output vector 
\begin_inset Formula $o$
\end_inset

 with the desired output vector 
\begin_inset Formula $d$
\end_inset

 to get the error value using the following calculation:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Error=(d-o)^{2}\label{eq:error}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The squared difference between the output vector 
\begin_inset Formula $o$
\end_inset

and the desired output vector 
\begin_inset Formula $d$
\end_inset

gives us the sense of how far the desired value for a particular input 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann"

\end_inset

.
 The back-propagation method aims to minimize the sum of all 
\begin_inset Formula $Error$
\end_inset

 value for all samples in the training data thus improving the performance
 of the network i.e.
 the ability to learn the relationship between the training input and the
 desired output 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Minimize(\sum(d-o)^{2})\label{eq:minimizeerror}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
By using a continuous function such as Pure-linear or Log sigmoid as the
 activation function (refer to 
\begin_inset CommandInset ref
LatexCommand formatted
reference "subsec:Artificial-Neuron"

\end_inset

 and 
\begin_inset CommandInset ref
LatexCommand formatted
reference "tab:Activation-functions"

\end_inset

), we can express the change of 
\begin_inset Formula $Error$
\end_inset

 value or gradient with respect to the change of weight vectors 
\begin_inset Formula $w$
\end_inset

 as follow 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann"

\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
Gradient=\frac{\delta Error}{\delta w}\label{eq:gradient}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
To update the weight vectors every time a training sample is fed into the
 network we will need to use the learning rule in formula 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:updateweight"

\end_inset

 which is based on the fact that if we decrease the value of weight vector
 
\begin_inset Formula $w$
\end_inset

 in the direction of the gradient, the 
\begin_inset Formula $Error$
\end_inset

 value will decrease as well 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann"

\end_inset

.
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
w_{new}=w_{old}-n\frac{\delta Error}{\delta w}\label{eq:updateweight}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Where:
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
\begin_inset Formula $w_{new}$
\end_inset

 is the newly updated weight vectors.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
\begin_inset Formula $w_{old}$
\end_inset

 is the old weight vectors.
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
\begin_inset Formula $n$
\end_inset

 is the learning rate which should be a small number (about 0.1)
\end_layout

\begin_layout Labeling
\labelwidthstring 00.00.0000
\begin_inset Formula $\frac{\delta Error}{\delta w}$
\end_inset

 is the gradient or the change of 
\begin_inset Formula $Error$
\end_inset

 value with respect to the change of weight vector 
\begin_inset Formula $w$
\end_inset

.
\end_layout

\begin_layout Standard
When the back-propagation cycle repeats for many epochs, by using the formula
 
\begin_inset CommandInset ref
LatexCommand eqref
reference "eq:updateweight"

\end_inset

, the weight vectors are constantly adjusted so that the 
\begin_inset Formula $Error$
\end_inset

 value decreases to a minimum value 
\begin_inset CommandInset citation
LatexCommand citep
key "web:ann"

\end_inset

.
 When the 
\begin_inset Formula $Error$
\end_inset

 value is at minimum, it often means that the network is trained and is
 ready to produce output similar to the desired output vector 
\begin_inset Formula $d$
\end_inset

 when it is presented with corresponding input vector 
\begin_inset Formula $i$
\end_inset

.
\end_layout

\end_body
\end_document
